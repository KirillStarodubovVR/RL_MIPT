Методы семейства «актор-критик»

Задача: обучить агента действовать в средах с непрерывным пространством действий, используя алгоритм SAC (Soft Actor Critic) или PPO (Proximal Policy Optimization).

**Описание**

Ваша задача — обучить агента в одной из следующих сред: Half Cheetah, Walker2D, Humanoid, Inverted Double Pendulum. Эти среды доступны в библиотеках gym, pybullet-gym или DeepMind Control Suite. Цель заключается в том, чтобы добиться хорошего результата в минимум трех из четырех указанных сред. Приветствуется проведение экспериментов с различными гиперпараметрами.

**Детали:**

* Рекомендуется использовать алгоритмы SAC или PPO для обучения.
* Проведите эксперименты с разными гиперпараметрами для достижения наилучших результатов.
* Пользуйтесь предоставленными ссылками и примерами для установки и использования необходимых библиотек.

**Что нужно сдать:**

* Код обучения в формате Jupyter.
* График сходимости, показывающий среднее вознаграждение.
* Веса обученной модели.
* Код для запуска модели с готовыми весами.
* Выводы по используемым гиперпараметрам.