Трансформеры принятия решений и заключение
Задача: провести эксперименты с алгоритмами автономного обучения с подкреплением в среде Pogema.

Описание

Настройка среды. Использовать предложенную конфигурацию:

grid_config = GridConfig(num_agents=1,  # number of agents
                        size=15, # size of the grid
                        density=0.4,  # obstacle density
                        seed=2,  # set to None for random
                                 # obstacles, agents and targets
                                 # positions at each reset
                        max_episode_steps=128,  # horizon
                        obs_radius=5,  # defines field of view
                        )
Улучшение сходимости алгоритма:

Добавьте wrapper, который будет выдавать агенту плотную награду за выполнение действий. К примеру пропорциональную изменению расстоянию от агента до цели.
Исследование алгоритмов на данных с оптимальной стратегией:

Использовать датасет с оптимальной стратегией поведения, полученной путем обучения online RL алгоритма.
Обучить агентов, используя алгоритмы CQL и DT из фреймворка d3rlpy.
Сравнить результаты с алгоритмом BC.
Работа с субоптимальной стратегией:

Сгенерировать датасет с субоптимальной стратегией с использованием библиотеки d3rlpy: добавить в датасет с хорошей стратегией обучения данные со случайной стратегией обучения.
Обучить агентов, используя алгоритмы CQL и DT из фреймворка d3rlpy.
Сравнить результаты с алгоритмом BC.
Что нужно сдать:
Код обучения в формате Jupyter.
Графики сходимости (среднее вознаграждение) для всех исследованных алгоритмов.
Выводы по используемым по обучению агента на оптимальных и неоптимальных данных.
Все эти материалы должны быть представлены в одном zip-архиве.
Критерии оценивания:
Качество кода: четкость, отсутствие ошибок и правильность реализации алгоритмов — 3 балла.
Представленные графики отражают корректное поведение алгоритмов и их сходимость — 4 балла.
Выводы по процессу обучения адекватные и аргументированные — 3 балла.


Важно!

При неправильной реализации вознаграждения от среды, есть шанс, что на субоптимальных данных агент не сойдется.
Важно, чтобы у агента не возникало желания ходить по лабиринту, увеличивая вознаграждение, то уменьшая то увеличивая расстояние до цели. На это также влияет корректность реализации wrapper’а вознаграждения.