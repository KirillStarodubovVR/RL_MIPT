**Обучение на основе функции полезности**
Задача: воспроизвести результаты DQN на двух играх Atari.

Описание. Ваша задача — воспроизвести результаты DQN на играх Pong и другой игре на ваш выбор из таблицы лекций или семинара. Важно, чтобы вы не выбрали игру, где результат человека значительно превосходит DQN. Учтите, что некоторые игры могут потребовать специфических методов, например, MontezumaRevenge.

Цель состоит в том, чтобы ваш агент достиг среднего вознаграждения, указанного в таблице для DQN, с учетом std dev.

Детали:
Воспользуйтесь техниками из оригинальной статьи DeepMind: replay buffer, frame skip, target network.
Можно использовать любой фреймворк.
Приветствуются улучшения базового кода или дополнительные техники.
Что нужно сдать:
Код обучения в формате Jupyter.
График сходимости, показывающий среднее вознаграждение.
Веса обученной модели.
Код для запуска модели с готовыми весами.
Выводы по используемым гиперпараметрам, таким как размер replay buffer, learning rate и т. д.
